{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Necessary imports and globals.\n",
    "import numpy as np\n",
    "import os\n",
    "from dopamine.agents.dqn import dqn_agent\n",
    "from dopamine.discrete_domains import run_experiment, atari_lib\n",
    "from dopamine.colab import utils as colab_utils\n",
    "from absl import flags\n",
    "import gin.tf\n",
    "\n",
    "BASE_PATH = 'running-data'  # @param\n",
    "GAME = 'BattleZone'  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data = colab_utils.load_baselines('./baselines-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = os.path.join(BASE_PATH, 'prioritized_srdqn', GAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\apple\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\apple\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'dopamine.discrete_domains.atari_lib' has no attribute 'SRNetwork'\n  In call to configurable 'DQNAgent' (<class 'dopamine.agents.dqn.dqn_agent.DQNAgent'>)\n  In call to configurable 'Runner' (<class 'dopamine.discrete_domains.run_experiment.Runner'>)\n  In call to configurable 'TrainRunner' (<class 'dopamine.discrete_domains.run_experiment.TrainRunner'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8751e717958d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;31m# to terminate quickly, as this is mostly meant for demonstrating how one can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;31m# use the framework.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m \u001b[0mprioritized_srdqn_runner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLOG_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_prioritized_srdqn_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1067\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" in scope '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m       \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\dopamine\\discrete_domains\\run_experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, base_dir, create_agent_fn, create_environment_fn)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Creating TrainRunner ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     super(TrainRunner, self).__init__(base_dir, create_agent_fn,\n\u001b[1;32m--> 553\u001b[1;33m                                       create_environment_fn)\n\u001b[0m\u001b[0;32m    554\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1067\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" in scope '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m       \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\dopamine\\discrete_domains\\run_experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, base_dir, create_agent_fn, create_environment_fn, checkpoint_file_prefix, logging_file_prefix, log_every_n, num_iterations, training_steps, evaluation_steps, max_steps_per_episode, clip_rewards)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     self._agent = create_agent_fn(self._sess, self._environment,\n\u001b[1;32m--> 219\u001b[1;33m                                   summary_writer=self._summary_writer)\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_summary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8751e717958d>\u001b[0m in \u001b[0;36mcreate_prioritized_srdqn_agent\u001b[1;34m(sess, environment, summary_writer)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_prioritized_srdqn_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m   \u001b[1;34m\"\"\"The Runner class will expect a function of this type to create an agent.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mPrioritizedSRDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m prioritized_srdqn_config = \"\"\"\n",
      "\u001b[1;32m<ipython-input-4-8751e717958d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sess, num_actions)\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;34m\"\"\"This maintains all the DQN default argument values.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replay_scheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'prioritized'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1067\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" in scope '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m       \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dopamine-need\\lib\\dopamine\\agents\\dqn\\dqn_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sess, num_actions, observation_shape, observation_dtype, stack_size, network, gamma, update_horizon, min_replay_history, update_period, target_update_period, epsilon_fn, epsilon_train, epsilon_eval, epsilon_decay_period, tf_device, eval_mode, use_staging, max_tf_checkpoints_to_keep, optimizer, summary_writer, summary_writing_frequency, allow_partial_reload)\u001b[0m\n\u001b[0;32m    195\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_replay_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_staging\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_networks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_train_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8751e717958d>\u001b[0m in \u001b[0;36m_build_networks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_convnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Online'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_convnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Target'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msr_convnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matari_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSRNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matari_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNATURE_DQN_STACK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_net_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_convnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_ph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# TODO(bellemare): Ties should be broken. They are unlikely to happen when\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dopamine.discrete_domains.atari_lib' has no attribute 'SRNetwork'\n  In call to configurable 'DQNAgent' (<class 'dopamine.agents.dqn.dqn_agent.DQNAgent'>)\n  In call to configurable 'Runner' (<class 'dopamine.discrete_domains.run_experiment.Runner'>)\n  In call to configurable 'TrainRunner' (<class 'dopamine.discrete_domains.run_experiment.TrainRunner'>)"
     ]
    }
   ],
   "source": [
    "# @title Create the DQN with prioritized replay\n",
    "from dopamine.replay_memory import prioritized_replay_buffer\n",
    "import tensorflow as tf\n",
    "\n",
    "class PrioritizedSRDQNAgent(dqn_agent.DQNAgent):\n",
    "  def __init__(self, sess, num_actions):\n",
    "    \"\"\"This maintains all the DQN default argument values.\"\"\"\n",
    "    super().__init__(sess, num_actions)\n",
    "    self._replay_scheme = 'prioritized'\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        self._sr_train_op = self._build_sr_train_op()\n",
    "\n",
    "  def _build_networks(self):\n",
    "    \"\"\"Builds the Q-value network computations needed for acting and training.\n",
    "\n",
    "    These are:\n",
    "      self.online_convnet: For computing the current state's Q-values.\n",
    "      self.target_convnet: For computing the next state's target Q-values.\n",
    "      self.sr_convnet: For computing the sr for state-action pair\n",
    "      self._net_outputs: The actual Q-values.\n",
    "      self._q_argmax: The action maximizing the current state's Q-values.\n",
    "      self._replay_net_outputs: The replayed states' Q-values.\n",
    "      self._replay_next_target_net_outputs: The replayed next states' target\n",
    "        Q-values (see Mnih et al., 2015 for details).\n",
    "    \"\"\"\n",
    "\n",
    "    # _network_template instantiates the model and returns the network object.\n",
    "    # The network object can be used to generate different outputs in the graph.\n",
    "    # At each call to the network, the parameters will be reused.\n",
    "    self.online_convnet = self._create_network(name='Online')\n",
    "    self.target_convnet = self._create_network(name='Target')\n",
    "    self.sr_convnet = atari_lib.SRNetwork(self.num_actions, atari_lib.NATURE_DQN_STACK_SIZE)\n",
    "    self._net_outputs = self.online_convnet(self.state_ph)\n",
    "    # TODO(bellemare): Ties should be broken. They are unlikely to happen when\n",
    "    # using a deep network, but may affect performance with a linear\n",
    "    # approximation scheme.\n",
    "    self._q_argmax = tf.argmax(self._net_outputs.q_values, axis=1)[0]\n",
    "    self._replay_net_outputs = self.online_convnet(self._replay.states)\n",
    "    \n",
    "    # sr for states sampled\n",
    "    self._sr_net_outputs = self.sr_convnet(self._replay.states, self._replay.actions)\n",
    "    # sr for next_states sampled\n",
    "    self._sr_net_outputs_next = self.sr_convnet(self._replay.next_states, self._replay.next_actions)\n",
    "    # sr for current state and action\n",
    "    self._sr_net_curr_state = self.sr_convnet(self.state_ph, self._q_argmax)\n",
    "    \n",
    "    self._replay_next_target_net_outputs = self.target_convnet(\n",
    "        self._replay.next_states)\n",
    "\n",
    "  def _build_replay_buffer(self, use_staging):\n",
    "    return prioritized_replay_buffer.WrappedPrioritizedReplayBuffer(\n",
    "        observation_shape=self.observation_shape,\n",
    "        stack_size=self.stack_size,\n",
    "        use_staging=use_staging,\n",
    "        update_horizon=self.update_horizon,\n",
    "        gamma=self.gamma,\n",
    "        observation_dtype=self.observation_dtype.as_numpy_dtype)\n",
    "  \n",
    "  def _build_sr_train_op(self):\n",
    "    feature = self._sr_net_outputs.feature\n",
    "    decoded_state = self._sr_net_outputs.decoded_state\n",
    "    sr_values = self._sr_net_outputs.sr_values\n",
    "    \n",
    "    loss_ae = tf.compat.v1.losses.huber_loss(\n",
    "        self._replay.states, decoded_state, reduction=tf.losses.Reduction.NONE\n",
    "    )\n",
    "    \n",
    "    loss_sr = tf.compat.v1.losses.mean_squared_error(\n",
    "        self._sr_net_outputs, feature + self.gamma * self._sr_net_outputs_next, weights=1.0, scope=None,\n",
    "        loss_collection=tf.GraphKeys.LOSSES, reduction=Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "    )\n",
    "    \n",
    "    loss = loss_ae + loss_sr\n",
    "    return self.optimizer.minimize(tf.reduce_mean(loss))\n",
    "\n",
    "  def _build_train_op(self):\n",
    "    \"\"\"Builds a training op.\n",
    "    Returns:\n",
    "      train_op: An op performing one step of training from replay data.\n",
    "    \"\"\"\n",
    "    replay_action_one_hot = tf.one_hot(\n",
    "        self._replay.actions, self.num_actions, 1., 0., name='action_one_hot')\n",
    "    replay_chosen_q = tf.reduce_sum(\n",
    "        self._replay_net_outputs.q_values * replay_action_one_hot,\n",
    "        axis=1,\n",
    "        name='replay_chosen_q')\n",
    "    \n",
    "    # output from the SR network\n",
    "    sr_values = self._sr_net_outputs.sr_values\n",
    "    curr_feature = self._sess.run(self._sr_net_curr_state, {self.state_ph: self.state}).feature\n",
    "    need = tf.tensordot(curr_feature, sr_values, axes=[[0], [1]])\n",
    "\n",
    "    target = tf.stop_gradient(self._build_target_q_op())\n",
    "    loss = tf.compat.v1.losses.huber_loss(\n",
    "        target, replay_chosen_q, reduction=tf.losses.Reduction.NONE)\n",
    "    # The original prioritized experience replay uses a linear exponent\n",
    "    # schedule 0.4 -> 1.0. Comparing the schedule to a fixed exponent of 0.5\n",
    "    # on 5 games (Asterix, Pong, Q*Bert, Seaquest, Space Invaders) suggested\n",
    "    # a fixed exponent actually performs better, except on Pong.\n",
    "    probs = self._replay.transition['sampling_probabilities']\n",
    "    loss_weights = 1.0 / tf.sqrt(probs + 1e-10)\n",
    "    loss_weights /= tf.reduce_max(loss_weights)\n",
    "\n",
    "    # Rainbow and prioritized replay are parametrized by an exponent alpha,\n",
    "    # but in both cases it is set to 0.5 - for simplicity's sake we leave it\n",
    "    # as is here, using the more direct tf.sqrt(). Taking the square root\n",
    "    # \"makes sense\", as we are dealing with a squared loss.\n",
    "    # Add a small nonzero value to the loss to avoid 0 priority items. While\n",
    "    # technically this may be okay, setting all items to 0 priority will cause\n",
    "    # troubles, and also result in 1.0 / 0.0 = NaN correction terms.\n",
    "    update_priorities_op = self._replay.tf_set_priority(\n",
    "        self._replay.indices, tf.sqrt(loss + 1e-10))\n",
    "\n",
    "    # Weight the loss by the inverse priorities.\n",
    "    loss = loss_weights * loss * need\n",
    "    \n",
    "    with tf.control_dependencies([update_priorities_op]):\n",
    "      if self.summary_writer is not None:\n",
    "        with tf.compat.v1.variable_scope('Losses'):\n",
    "          tf.compat.v1.summary.scalar('HuberLoss', tf.reduce_mean(loss))\n",
    "      return self.optimizer.minimize(tf.reduce_mean(loss))\n",
    "\n",
    "  def _store_transition(self,\n",
    "                        last_observation,\n",
    "                        action,\n",
    "                        reward,\n",
    "                        is_terminal,\n",
    "                        priority=None):\n",
    "    priority = self._replay.memory.sum_tree.max_recorded_priority\n",
    "    if not self.eval_mode:\n",
    "      self._replay.add(last_observation, action, reward, is_terminal, priority)\n",
    "    \n",
    "  def _record_observation(self, observation):\n",
    "    \"\"\"Records an observation and update state.\n",
    "\n",
    "    Extracts a frame from the observation vector and overwrites the oldest\n",
    "    frame in the state buffer.\n",
    "\n",
    "    Args:\n",
    "      observation: numpy array, an observation from the environment.\n",
    "    \"\"\"\n",
    "    # Set current observation. We do the reshaping to handle environments\n",
    "    # without frame stacking.\n",
    "    self._observation = np.reshape(observation, self.observation_shape)\n",
    "    # Swap out the oldest frame with the current frame.\n",
    "    self.state = np.roll(self.state, -1, axis=-1)\n",
    "    print(self.state.shape)\n",
    "    self.state[0, ..., -1] = self._observation\n",
    "    \n",
    "  def _train_step(self):\n",
    "    \"\"\"Runs a single training step.\n",
    "\n",
    "    Runs a training op if both:\n",
    "      (1) A minimum number of frames have been added to the replay buffer.\n",
    "      (2) `training_steps` is a multiple of `update_period`.\n",
    "\n",
    "    Also, syncs weights from online to target network if training steps is a\n",
    "    multiple of target update period.\n",
    "    \"\"\"\n",
    "    # Run a train op at the rate of self.update_period if enough training steps\n",
    "    # have been run. This matches the Nature DQN behaviour.\n",
    "    if self._replay.memory.add_count > self.min_replay_history:\n",
    "      if self.training_steps % self.update_period == 0:\n",
    "        self._sess.run(self._train_op)\n",
    "        self._sess.run(self._sr_train_op)\n",
    "        if (self.summary_writer is not None and\n",
    "            self.training_steps > 0 and\n",
    "            self.training_steps % self.summary_writing_frequency == 0):\n",
    "          summary = self._sess.run(self._merged_summaries)\n",
    "          self.summary_writer.add_summary(summary, self.training_steps)\n",
    "\n",
    "      if self.training_steps % self.target_update_period == 0:\n",
    "        self._sess.run(self._sync_qt_ops)\n",
    "\n",
    "    self.training_steps += 1\n",
    "    \n",
    "def create_prioritized_srdqn_agent(sess, environment, summary_writer=None):\n",
    "  \"\"\"The Runner class will expect a function of this type to create an agent.\"\"\"\n",
    "  return PrioritizedSRDQNAgent(sess, num_actions=environment.action_space.n)\n",
    "\n",
    "prioritized_srdqn_config = \"\"\"\n",
    "import dopamine.discrete_domains.atari_lib\n",
    "import dopamine.discrete_domains.run_experiment\n",
    "import dopamine.agents.dqn.dqn_agent\n",
    "import dopamine.replay_memory.prioritized_replay_buffer\n",
    "import gin.tf.external_configurables\n",
    "\n",
    "DQNAgent.gamma = 0.99\n",
    "DQNAgent.update_horizon = 1\n",
    "DQNAgent.min_replay_history = 20000  # agent steps\n",
    "DQNAgent.update_period = 4\n",
    "DQNAgent.target_update_period = 8000  # agent steps\n",
    "DQNAgent.epsilon_train = 0.01\n",
    "DQNAgent.epsilon_eval = 0.001\n",
    "DQNAgent.epsilon_decay_period = 250000  # agent steps\n",
    "DQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
    "DQNAgent.optimizer = @tf.train.RMSPropOptimizer()\n",
    "\n",
    "tf.train.RMSPropOptimizer.learning_rate = 0.00025\n",
    "tf.train.RMSPropOptimizer.decay = 0.95\n",
    "tf.train.RMSPropOptimizer.momentum = 0.0\n",
    "tf.train.RMSPropOptimizer.epsilon = 0.00001\n",
    "tf.train.RMSPropOptimizer.centered = True\n",
    "\n",
    "atari_lib.create_atari_environment.game_name = '{}'\n",
    "# Sticky actions with probability 0.25, as suggested by (Machado et al., 2017).\n",
    "atari_lib.create_atari_environment.sticky_actions = True\n",
    "create_agent.agent_name = 'dqn'\n",
    "Runner.num_iterations = 200\n",
    "Runner.training_steps = 250000  # agent steps\n",
    "Runner.evaluation_steps = 125000  # agent steps\n",
    "Runner.max_steps_per_episode = 27000  # agent steps\n",
    "\n",
    "WrappedPrioritizedReplayBuffer.replay_capacity = 1000000\n",
    "WrappedPrioritizedReplayBuffer.batch_size = 32\n",
    "\"\"\".format(GAME)\n",
    "gin.parse_config(prioritized_srdqn_config, skip_unknown=False)\n",
    "\n",
    "# Create the runner class with this agent. We use very small numbers of steps\n",
    "# to terminate quickly, as this is mostly meant for demonstrating how one can\n",
    "# use the framework.\n",
    "prioritized_srdqn_runner = run_experiment.TrainRunner(LOG_PATH, create_prioritized_srdqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train MyRandomDQNAgent.\n",
    "print('Will train agent, please be patient, may be a while...')\n",
    "prioritized_srdqn_runner.run_experiment()\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tf.constant([1.5, 2.5, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(3,) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.constant([[1.5, 2.5, 3.5], [1.5, 2.5, 3.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=(2, 3) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = tf.constant([[[1.5, 2.5, 3.5], [1.5, 2.5, 3.5]], [[1.5, 2.5, 3.5], [1.5, 2.5, 3.5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(2, 2, 3) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Tensordot_4:0' shape=(2,) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tensordot(t1, t2, axes=[[0], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dopamine-need",
   "language": "python",
   "name": "dopamine-need"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
