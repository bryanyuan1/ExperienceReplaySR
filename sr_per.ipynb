{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Necessary imports and globals.\n",
    "import numpy as np\n",
    "import os\n",
    "import dopamine\n",
    "from dopamine.agents.dqn import dqn_agent\n",
    "from dopamine.discrete_domains import run_experiment, atari_lib\n",
    "from dopamine.colab import utils as colab_utils\n",
    "from absl import flags\n",
    "import gin.tf\n",
    "\n",
    "BASE_PATH = 'running-data'  # @param\n",
    "GAME = 'BattleZone'  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_data = colab_utils.load_baselines('./baselines-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = os.path.join(BASE_PATH, 'prioritized_srdqn', GAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "SRNetworkType = collections.namedtuple('sr_network', ['feature', 'decoded_state', 'sr_values'])\n",
    "\n",
    "class SRNetwork(tf.keras.Model):\n",
    "  \"\"\"The convolutional network used to compute the agent's successor features\"\"\"\n",
    "\n",
    "  def __init__(self, num_actions, in_channnels, name=None):\n",
    "    \"\"\"Creates the layers used for calculating Q-values.\n",
    "\n",
    "    Args:\n",
    "      num_actions: int, number of actions.\n",
    "      name: str, used to create scope for network parameters.\n",
    "    \"\"\"\n",
    "    super(SRNetwork, self).__init__(name=name)\n",
    "\n",
    "    self.num_actions = num_actions\n",
    "    self.in_channels = in_channnels\n",
    "\n",
    "    # Defining layers.\n",
    "    activation_fn = tf.keras.activations.relu\n",
    "    # Setting names of the layers manually to make variable names more similar\n",
    "    # with tf.slim variable names/checkpoints.\n",
    "    self.conv1 = tf.keras.layers.Conv2D(32, [8, 8], strides=4, padding='same',\n",
    "                                        activation=activation_fn, name='Conv')\n",
    "    self.conv2 = tf.keras.layers.Conv2D(64, [6, 6], strides=3, padding='same',\n",
    "                                        activation=activation_fn, name='Conv')\n",
    "    self.conv3 = tf.keras.layers.Conv2D(64, [3, 3], strides=1, padding='same',\n",
    "                                        activation=activation_fn, name='Conv')\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.dense_phi = tf.keras.layers.Dense(512, name='fully_connected') # the phi_st\n",
    "\n",
    "    # auto-decoder to reconstruct state\n",
    "    self.dense_decoder = tf.keras.layers.Dense(7 * 7 * self.in_channels, activation=activation_fn, name='dense_decoder')\n",
    "    self.reshape_decoder = tf.keras.layers.Reshape((7, 7, self.in_channels))\n",
    "    self.conv4 = tf.keras.layers.Conv2DTranspose(64, [3, 3], strides=1, padding='same',\n",
    "                                        activation=activation_fn, name='Conv')\n",
    "    self.conv5 = tf.keras.layers.Conv2DTranspose(64, [6, 6], strides=3, padding='same',\n",
    "                                        activation=activation_fn, name='Conv')\n",
    "    self.conv6 = tf.keras.layers.Conv2DTranspose(32, [8, 8], strides=4, padding='same',\n",
    "                                        activation=activation_fn, name='Conv')\n",
    "    self.conv_st = tf.keras.layers.Conv2D(self.in_channels, [4, 4], strides=1, padding='same',\n",
    "                                        name='Conv') # the output s^t\n",
    "\n",
    "    # successor representation branches\n",
    "    self.branches = [[\n",
    "        tf.keras.layers.Dense(512, name='fully_connected',\n",
    "                              activation=activation_fn),\n",
    "        tf.keras.layers.Dense(256, name='fully_connected',\n",
    "                              activation=activation_fn),\n",
    "        tf.keras.layers.Dense(512, name='fully_connected')\n",
    "    ] for i in range(self.num_actions)]\n",
    "\n",
    "  def call(self, state):\n",
    "    \"\"\"Creates the output tensor/op given the state tensor as input.\n",
    "\n",
    "    See https://www.tensorflow.org/api_docs/python/tf/keras/Model for more\n",
    "    information on this. Note that tf.keras.Model implements `call` which is\n",
    "    wrapped by `__call__` function by tf.keras.Model.\n",
    "\n",
    "    Parameters created here will have scope according to the `name` argument\n",
    "    given at `.__init__()` call.\n",
    "    Args:\n",
    "      state: Tensor, input tensor.\n",
    "    Returns:\n",
    "      collections.namedtuple, output ops (graph mode) or output tensors (eager).\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute feature\n",
    "    x = tf.cast(state, tf.float32)\n",
    "    x = x / 255\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.flatten(x)\n",
    "    phi = self.dense_phi(x)\n",
    "\n",
    "    # compute decoded feature\n",
    "    phi_st = self.dense_decoder(phi)\n",
    "    phi_st = self.reshape_decoder(phi_st)\n",
    "    phi_st = self.conv4(phi_st)\n",
    "    phi_st = self.conv5(phi_st)\n",
    "    phi_st = self.conv6(phi_st)\n",
    "    phi_st = self.conv_st(phi_st)\n",
    "    decoded_state = tf.multiply(phi_st, 255)\n",
    "\n",
    "    # compute successor representation\n",
    "    srs = []\n",
    "    for i in range(self.num_actions):\n",
    "      branch = self.branches[i]\n",
    "      sr = branch[0](phi)\n",
    "      sr = branch[1](sr)\n",
    "      sr = branch[2](sr)\n",
    "      srs.append(sr)\n",
    "    srs = tf.convert_to_tensor(srs)\n",
    "\n",
    "    return SRNetworkType(phi, decoded_state, srs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished constructing\n",
      "Model: \"Online\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv (Conv2D)                multiple                  8224      \n",
      "_________________________________________________________________\n",
      "Conv (Conv2D)                multiple                  32832     \n",
      "_________________________________________________________________\n",
      "Conv (Conv2D)                multiple                  36928     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  3965440   \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  9234      \n",
      "=================================================================\n",
      "Total params: 4,052,658\n",
      "Trainable params: 4,052,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sr_network_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv (Conv2D)                multiple                  8224      \n",
      "_________________________________________________________________\n",
      "Conv (Conv2D)                multiple                  73792     \n",
      "_________________________________________________________________\n",
      "Conv (Conv2D)                multiple                  36928     \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  1606144   \n",
      "_________________________________________________________________\n",
      "dense_decoder (Dense)        multiple                  100548    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "Conv (Conv2DTranspose)       multiple                  2368      \n",
      "_________________________________________________________________\n",
      "Conv (Conv2DTranspose)       multiple                  147520    \n",
      "_________________________________________________________________\n",
      "Conv (Conv2DTranspose)       multiple                  131104    \n",
      "_________________________________________________________________\n",
      "Conv (Conv2D)                multiple                  2052      \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  262656    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131328    \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      multiple                  131584    \n",
      "=================================================================\n",
      "Total params: 11,568,904\n",
      "Trainable params: 11,568,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# @title Create the DQN with prioritized replay\n",
    "from dopamine.replay_memory import prioritized_replay_buffer\n",
    "import tensorflow as tf\n",
    "\n",
    "class PrioritizedSRDQNAgent(dqn_agent.DQNAgent):\n",
    "  def __init__(self, sess, num_actions):\n",
    "    \"\"\"This maintains all the DQN default argument values.\"\"\"\n",
    "    super().__init__(sess, num_actions, tf_device='/gpu:0')\n",
    "    self._replay_scheme = 'prioritized'\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        self._build_networks()\n",
    "\n",
    "    with tf.device('/gpu:1'):\n",
    "        self._train_op = self._build_train_op()\n",
    "        self._sr_train_op = self._build_sr_train_op()\n",
    "        self._sync_qt_ops = self._build_sync_op()\n",
    "        \n",
    "    print('finished constructing')\n",
    "    self.online_convnet.summary()\n",
    "    self.sr_convnet.summary()\n",
    "    \n",
    "        \n",
    "\n",
    "  def _build_networks(self):\n",
    "    \"\"\"Builds the Q-value network computations needed for acting and training.\n",
    "\n",
    "    These are:\n",
    "      self.online_convnet: For computing the current state's Q-values.\n",
    "      self.target_convnet: For computing the next state's target Q-values.\n",
    "      self.sr_convnet: For computing the sr for state-action pair\n",
    "      self._net_outputs: The actual Q-values.\n",
    "      self._q_argmax: The action maximizing the current state's Q-values.\n",
    "      self._replay_net_outputs: The replayed states' Q-values.\n",
    "      self._replay_next_target_net_outputs: The replayed next states' target\n",
    "        Q-values (see Mnih et al., 2015 for details).\n",
    "    \"\"\"\n",
    "\n",
    "    # _network_template instantiates the model and returns the network object.\n",
    "    # The network object can be used to generate different outputs in the graph.\n",
    "    # At each call to the network, the parameters will be reused.\n",
    "    self.online_convnet = self._create_network(name='Online')\n",
    "    self.target_convnet = self._create_network(name='Target')\n",
    "    self._net_outputs = self.online_convnet(self.state_ph)\n",
    "    # TODO(bellemare): Ties should be broken. They are unlikely to happen when\n",
    "    # using a deep network, but may affect performance with a linear\n",
    "    # approximation scheme.\n",
    "    self._q_argmax = tf.argmax(self._net_outputs.q_values, axis=1)[0]\n",
    "    self._replay_net_outputs = self.online_convnet(self._replay.transition['state'])\n",
    "    self._replay_next_target_net_outputs = self.target_convnet(\n",
    "        self._replay.transition['next_state'])\n",
    "    \n",
    "    self._q_argmax_sr = tf.argmax(self._net_outputs.q_values, axis=1)\n",
    "    self.sr_convnet = SRNetwork(self.num_actions, atari_lib.NATURE_DQN_STACK_SIZE)\n",
    "    # sr for states sampled\n",
    "    self._sr_net_outputs = self.sr_convnet(self._replay.transition['state'])\n",
    "    # sr for next_states sampled\n",
    "    self._sr_net_outputs_next = self.sr_convnet(self._replay.transition['next_state'])\n",
    "    # sr for current state and action\n",
    "    self._sr_net_curr_state = self.sr_convnet(self.state_ph)\n",
    "\n",
    "  def _build_replay_buffer(self, use_staging):\n",
    "    return prioritized_replay_buffer.WrappedPrioritizedReplayBuffer(\n",
    "        observation_shape=self.observation_shape,\n",
    "        stack_size=self.stack_size,\n",
    "        use_staging=use_staging,\n",
    "        update_horizon=self.update_horizon,\n",
    "        gamma=self.gamma,\n",
    "        observation_dtype=self.observation_dtype.as_numpy_dtype)\n",
    "  \n",
    "  def _build_sr_train_op(self):\n",
    "    feature = self._sr_net_outputs.feature\n",
    "    decoded_state = self._sr_net_outputs.decoded_state\n",
    "    \n",
    "    loss_ae = tf.compat.v1.losses.huber_loss(\n",
    "        self._replay.states, decoded_state, reduction=tf.losses.Reduction.NONE\n",
    "    )\n",
    "    srs = self._sr_net_outputs.sr_values\n",
    "    indices = tf.transpose(tf.stack([self._replay.actions, tf.constant([i for i in range(32)])]))\n",
    "    srs = tf.gather_nd(srs, indices)\n",
    "    \n",
    "    srs_next = self._sr_net_outputs_next.sr_values\n",
    "    indices_next = tf.transpose(tf.stack([self._replay.next_actions, tf.constant([i for i in range(32)])]))\n",
    "    srs_next = tf.gather_nd(srs_next, indices_next)\n",
    "    \n",
    "    assert feature.shape == srs_next.shape\n",
    "    assert srs.shape == feature.shape\n",
    "    \n",
    "    loss_sr = tf.compat.v1.losses.mean_squared_error(\n",
    "        srs, feature + self.gamma * srs_next\n",
    "    )\n",
    "    loss = loss_ae + loss_sr\n",
    "    return self.optimizer.minimize(tf.reduce_mean(loss))\n",
    "\n",
    "  def _build_train_op(self):\n",
    "    \"\"\"Builds a training op.\n",
    "    Returns:\n",
    "      train_op: An op performing one step of training from replay data.\n",
    "    \"\"\"\n",
    "    replay_action_one_hot = tf.one_hot(\n",
    "        self._replay.actions, self.num_actions, 1., 0., name='action_one_hot')\n",
    "    replay_chosen_q = tf.reduce_sum(\n",
    "        self._replay_net_outputs.q_values * replay_action_one_hot,\n",
    "        axis=1,\n",
    "        name='replay_chosen_q')\n",
    "    \n",
    "    # output from the SR network\n",
    "    # note that the back prop of the q-loss should not take into account\n",
    "    # the graph of the need term.\n",
    "    curr_action = tf.stop_gradient(self._q_argmax_sr)\n",
    "    sample_features = self._sr_net_outputs.feature\n",
    "    curr_sr = self._sr_net_curr_state.sr_values    \n",
    "    curr_sr = tf.gather_nd(curr_sr, curr_action)\n",
    "    need = tf.stop_gradient(\n",
    "        tf.tensordot(curr_sr, sample_features, axes=[[1], [1]])[0]\n",
    "    )\n",
    "    \n",
    "\n",
    "    target = tf.stop_gradient(self._build_target_q_op())\n",
    "    loss = tf.compat.v1.losses.huber_loss(\n",
    "        target, replay_chosen_q, reduction=tf.losses.Reduction.NONE)\n",
    "    # The original prioritized experience replay uses a linear exponent\n",
    "    # schedule 0.4 -> 1.0. Comparing the schedule to a fixed exponent of 0.5\n",
    "    # on 5 games (Asterix, Pong, Q*Bert, Seaquest, Space Invaders) suggested\n",
    "    # a fixed exponent actually performs better, except on Pong.\n",
    "    probs = self._replay.transition['sampling_probabilities']\n",
    "    loss_weights = 1.0 / tf.sqrt(probs + 1e-10)\n",
    "    loss_weights /= tf.reduce_max(loss_weights)\n",
    "\n",
    "    # Rainbow and prioritized replay are parametrized by an exponent alpha,\n",
    "    # but in both cases it is set to 0.5 - for simplicity's sake we leave it\n",
    "    # as is here, using the more direct tf.sqrt(). Taking the square root\n",
    "    # \"makes sense\", as we are dealing with a squared loss.\n",
    "    # Add a small nonzero value to the loss to avoid 0 priority items. While\n",
    "    # technically this may be okay, setting all items to 0 priority will cause\n",
    "    # troubles, and also result in 1.0 / 0.0 = NaN correction terms.\n",
    "    update_priorities_op = self._replay.tf_set_priority(\n",
    "        self._replay.indices, tf.sqrt(loss + 1e-10))\n",
    "\n",
    "    # Weight the loss by the inverse priorities.\n",
    "#     loss = loss_weights * loss * need\n",
    "    loss = loss_weights * loss\n",
    "    \n",
    "    assert need.shape == loss.shape\n",
    "    \n",
    "    # TODO: scheme 1: if need smaller than 0, move all need values up\n",
    "    need = need - tf.minimum(tf.math.reduce_min(need), 0.0)\n",
    "        \n",
    "    loss_need = need * loss\n",
    "    \n",
    "    with tf.control_dependencies([update_priorities_op]):\n",
    "      if self.summary_writer is not None:\n",
    "        with tf.compat.v1.variable_scope('Losses'):\n",
    "          tf.compat.v1.summary.scalar('HuberLoss', tf.reduce_mean(loss_need))\n",
    "      return self.optimizer.minimize(tf.reduce_mean(loss_need))\n",
    "\n",
    "  def _store_transition(self,\n",
    "                        last_observation,\n",
    "                        action,\n",
    "                        reward,\n",
    "                        is_terminal,\n",
    "                        priority=None):\n",
    "    priority = self._replay.memory.sum_tree.max_recorded_priority\n",
    "    if not self.eval_mode:\n",
    "      self._replay.add(last_observation, action, reward, is_terminal, priority)\n",
    "    \n",
    "  def _record_observation(self, observation):\n",
    "    \"\"\"Records an observation and update state.\n",
    "\n",
    "    Extracts a frame from the observation vector and overwrites the oldest\n",
    "    frame in the state buffer.\n",
    "\n",
    "    Args:\n",
    "      observation: numpy array, an observation from the environment.\n",
    "    \"\"\"\n",
    "    # Set current observation. We do the reshaping to handle environments\n",
    "    # without frame stacking.\n",
    "    self._observation = np.reshape(observation, self.observation_shape)\n",
    "    # Swap out the oldest frame with the current frame.\n",
    "    self.state = np.roll(self.state, -1, axis=-1)\n",
    "    self.state[0, ..., -1] = self._observation\n",
    "    \n",
    "  def _train_step(self):\n",
    "    \"\"\"Runs a single training step.\n",
    "\n",
    "    Runs a training op if both:\n",
    "      (1) A minimum number of frames have been added to the replay buffer.\n",
    "      (2) `training_steps` is a multiple of `update_period`.\n",
    "\n",
    "    Also, syncs weights from online to target network if training steps is a\n",
    "    multiple of target update period.\n",
    "    \"\"\"\n",
    "    # Run a train op at the rate of self.update_period if enough training steps\n",
    "    # have been run. This matches the Nature DQN behaviour.\n",
    "    if self._replay.memory.add_count > self.min_replay_history:\n",
    "      if self.training_steps % self.update_period == 0:\n",
    "        self._sess.run(self._train_op, {self.state_ph: self.state})\n",
    "        \n",
    "#         print(self._sess.run(self._sr_net_curr_state.feature, {self.state_ph: self.state}))\n",
    "        self._sess.run(self._sr_train_op)\n",
    "        if (self.summary_writer is not None and\n",
    "            self.training_steps > 0 and\n",
    "            self.training_steps % self.summary_writing_frequency == 0):\n",
    "          summary = self._sess.run(self._merged_summaries)\n",
    "          self.summary_writer.add_summary(summary, self.training_steps)\n",
    "\n",
    "      if self.training_steps % self.target_update_period == 0:\n",
    "        self._sess.run(self._sync_qt_ops)\n",
    "\n",
    "    self.training_steps += 1\n",
    "    \n",
    "def create_prioritized_srdqn_agent(sess, environment, summary_writer=None):\n",
    "  \"\"\"The Runner class will expect a function of this type to create an agent.\"\"\"\n",
    "  return PrioritizedSRDQNAgent(sess, num_actions=environment.action_space.n)\n",
    "\n",
    "prioritized_srdqn_config = \"\"\"\n",
    "import dopamine.discrete_domains.atari_lib\n",
    "import dopamine.discrete_domains.run_experiment\n",
    "import dopamine.agents.dqn.dqn_agent\n",
    "import dopamine.replay_memory.prioritized_replay_buffer\n",
    "import gin.tf.external_configurables\n",
    "\n",
    "DQNAgent.gamma = 0.99\n",
    "DQNAgent.update_horizon = 1\n",
    "DQNAgent.min_replay_history = 20000  # agent steps\n",
    "DQNAgent.update_period = 4\n",
    "DQNAgent.target_update_period = 8000  # agent steps\n",
    "DQNAgent.epsilon_train = 0.01\n",
    "DQNAgent.epsilon_eval = 0.001\n",
    "DQNAgent.epsilon_decay_period = 250000  # agent steps\n",
    "DQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
    "DQNAgent.optimizer = @tf.train.RMSPropOptimizer()\n",
    "\n",
    "tf.train.RMSPropOptimizer.learning_rate = 0.00025\n",
    "tf.train.RMSPropOptimizer.decay = 0.95\n",
    "tf.train.RMSPropOptimizer.momentum = 0.0\n",
    "tf.train.RMSPropOptimizer.epsilon = 0.00001\n",
    "tf.train.RMSPropOptimizer.centered = True\n",
    "\n",
    "atari_lib.create_atari_environment.game_name = '{}'\n",
    "# Sticky actions with probability 0.25, as suggested by (Machado et al., 2017).\n",
    "atari_lib.create_atari_environment.sticky_actions = True\n",
    "create_agent.agent_name = 'dqn'\n",
    "Runner.num_iterations = 200\n",
    "Runner.training_steps = 250000  # agent steps\n",
    "Runner.evaluation_steps = 125000  # agent steps\n",
    "Runner.max_steps_per_episode = 27000  # agent steps\n",
    "\n",
    "WrappedPrioritizedReplayBuffer.replay_capacity = 1000000\n",
    "WrappedPrioritizedReplayBuffer.batch_size = 32\n",
    "\"\"\".format(GAME)\n",
    "gin.parse_config(prioritized_srdqn_config, skip_unknown=False)\n",
    "\n",
    "# Create the runner class with this agent. We use very small numbers of steps\n",
    "# to terminate quickly, as this is mostly meant for demonstrating how one can\n",
    "# use the framework.\n",
    "prioritized_srdqn_runner = run_experiment.TrainRunner(LOG_PATH, create_prioritized_srdqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train agent, please be patient, may be a while...\n",
      "Steps executed: 38451 Episode length: 1825 Return: 5000.0\r"
     ]
    }
   ],
   "source": [
    "# @title Train MyRandomDQNAgent.\n",
    "print('Will train agent, please be patient, may be a while...')\n",
    "prioritized_srdqn_runner.run_experiment()\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tf.constant([0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.constant([[[-100, -1], [-1, -1], [-1, -1], [-1, -1]], [[-1, -1], [-1, -1], [-1, -1000], [-1, -1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -100,    -1],\n",
       "        [   -1,    -1],\n",
       "        [   -1,    -1],\n",
       "        [   -1,    -1]],\n",
       "\n",
       "       [[   -1,    -1],\n",
       "        [   -1,    -1],\n",
       "        [   -1, -1000],\n",
       "        [   -1,    -1]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = tf.transpose(t2)\n",
    "# t3 = tf.gather_nd(t2, [[1, 0], [1, 1], [1,2]])\n",
    "tf.keras.backend.get_value(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-100,   -1],\n",
       "       [  -1,   -1],\n",
       "       [  -1,   -1],\n",
       "       [  -1,   -1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.get_value(tf.gather_nd(t2, t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'one_hot_3:0' shape=(3, 3) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot([1, 2, 0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = tf.constant([i for i in range(32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = tf.constant([0 for i in range(32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 1,  0],\n",
       "       [ 2,  0],\n",
       "       [ 3,  0],\n",
       "       [ 4,  0],\n",
       "       [ 5,  0],\n",
       "       [ 6,  0],\n",
       "       [ 7,  0],\n",
       "       [ 8,  0],\n",
       "       [ 9,  0],\n",
       "       [10,  0],\n",
       "       [11,  0],\n",
       "       [12,  0],\n",
       "       [13,  0],\n",
       "       [14,  0],\n",
       "       [15,  0],\n",
       "       [16,  0],\n",
       "       [17,  0],\n",
       "       [18,  0],\n",
       "       [19,  0],\n",
       "       [20,  0],\n",
       "       [21,  0],\n",
       "       [22,  0],\n",
       "       [23,  0],\n",
       "       [24,  0],\n",
       "       [25,  0],\n",
       "       [26,  0],\n",
       "       [27,  0],\n",
       "       [28,  0],\n",
       "       [29,  0],\n",
       "       [30,  0],\n",
       "       [31,  0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.get_value(tf.transpose(tf.stack([all, ind])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dopamine-need",
   "language": "python",
   "name": "dopamine-need"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
